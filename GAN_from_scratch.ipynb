{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "nK7zlMflZC3x"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the MNIST dataset\n",
        "(train_images, train_labels), (_, _) = mnist.load_data()\n",
        "\n",
        "# Normalize the images\n",
        "train_images = train_images.astype(\"float32\") / 255.0\n",
        "\n",
        "# Reshape the images to (batch_size, height, width, channels)\n",
        "train_images = np.expand_dims(train_images, axis=3)\n",
        "\n",
        "# Set the dimensions of the noise vector\n",
        "latent_dim = 100"
      ],
      "metadata": {
        "id": "oh5GzLyVZFNB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the generator\n",
        "generator = models.Sequential()\n",
        "generator.add(layers.Dense(7 * 7 * 256, use_bias=False, input_shape=(latent_dim,)))\n",
        "generator.add(layers.BatchNormalization())\n",
        "generator.add(layers.LeakyReLU())\n",
        "\n",
        "generator.add(layers.Reshape((7, 7, 256)))\n",
        "generator.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
        "generator.add(layers.BatchNormalization())\n",
        "generator.add(layers.LeakyReLU())\n",
        "\n",
        "generator.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
        "generator.add(layers.BatchNormalization())\n",
        "generator.add(layers.LeakyReLU())\n",
        "\n",
        "generator.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='sigmoid'))\n"
      ],
      "metadata": {
        "id": "ye1qXj1BZKbx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the discriminator\n",
        "discriminator = models.Sequential()\n",
        "discriminator.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=(28, 28, 1)))\n",
        "discriminator.add(layers.LeakyReLU())\n",
        "discriminator.add(layers.Dropout(0.3))\n",
        "\n",
        "discriminator.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
        "discriminator.add(layers.LeakyReLU())\n",
        "discriminator.add(layers.Dropout(0.3))\n",
        "\n",
        "discriminator.add(layers.Flatten())\n",
        "discriminator.add(layers.Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "dY3VtzovZPMZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the models\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "generator.compile(loss='binary_crossentropy', optimizer='adam')"
      ],
      "metadata": {
        "id": "73T79ME7ZR0W"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the GAN\n",
        "gan = models.Sequential()\n",
        "gan.add(generator)\n",
        "gan.add(discriminator)\n",
        "gan.compile(loss='binary_crossentropy', optimizer='adam')"
      ],
      "metadata": {
        "id": "93jb7_9TZVq1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the GAN\n",
        "batch_size = 128\n",
        "epochs = 10\n",
        "num_samples = 20\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for batch in range(train_images.shape[0] // batch_size):\n",
        "        # Generate random noise as input to the generator\n",
        "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "\n",
        "        # Generate fake images using the generator\n",
        "        fake_images = generator.predict(noise)\n",
        "\n",
        "        # Select a random batch of real images\n",
        "        real_images = train_images[np.random.randint(0, train_images.shape[0], size=batch_size)]\n",
        "\n",
        "        # Train the discriminator on the real and fake images\n",
        "        discriminator_loss_real = discriminator.train_on_batch(real_images, np.ones((batch_size, 1)))\n",
        "        discriminator_loss_fake = discriminator.train_on_batch(fake_images, np.zeros((batch_size, 1)))\n",
        "        discriminator_loss = 0.5 * np.add(discriminator_loss_real, discriminator_loss_fake)\n",
        "\n",
        "        # Train the generator\n",
        "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "        generator_loss = gan.train_on_batch(noise, np.ones((batch_size, 1)))\n",
        "\n",
        "        # Print the losses every 10 epochs\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "\n",
        "          print(f\"Epoch {epoch + 1}, Discriminator Loss: {discriminator_loss}, Generator Loss: {generator_loss}\")"
      ],
      "metadata": {
        "id": "zcqlDtK_WQXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a sample of synthetic images to evaluate the generator\n",
        "noise = np.random.normal(0, 1, (num_samples, latent_dim))\n",
        "generated_images = generator.predict(noise)\n",
        "generated_images = generated_images.reshape((num_samples, 28, 28))\n",
        "\n",
        "# Plot the synthetic images\n",
        "plt.figure(figsize=(10, 10))\n",
        "for i in range(num_samples):\n",
        "    plt.subplot(10, 10, i + 1)\n",
        "    plt.imshow(generated_images[i], cmap='gray')\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "GZlznoPAX7vW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}